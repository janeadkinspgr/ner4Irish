{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VSIvgIVaMg-S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train = '/content/train_final.conll'\n",
        "\n",
        "def read_conll_file(file_path):\n",
        "    data = []\n",
        "    current_sentence = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if line.startswith('-DOCSTART-'):\n",
        "                continue\n",
        "            if line:\n",
        "                parts = line.split()\n",
        "                word = parts[0]\n",
        "                ner_label = parts[-1]\n",
        "                current_sentence.append((word, ner_label))\n",
        "            else:\n",
        "                if current_sentence:\n",
        "                    data.append(current_sentence)\n",
        "                    current_sentence = []\n",
        "    if current_sentence:\n",
        "        data.append(current_sentence)\n",
        "    return data\n",
        "\n",
        "train_data = read_conll_file(train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def augment_sentence_with_entities_v3(sentence, entity_pool, num_entities=1):\n",
        "    augmented_sentence = sentence[:]  # Make a copy of the sentence to preserve the original data.\n",
        "\n",
        "    # Determine insertion positions excluding the last position unless it's 'O'.\n",
        "    insert_positions = [i for i, (_, label) in enumerate(sentence[:-1]) if label == 'O' and sentence[i+1][1] == 'O']\n",
        "    if sentence[-1][1] == 'O':\n",
        "        insert_positions.append(len(sentence)-1)  # Include the last position if it's 'O'.\n",
        "\n",
        "    for _ in range(num_entities):\n",
        "        if not insert_positions:\n",
        "            break  # Exit if no suitable position is found.\n",
        "\n",
        "        insert_position = random.choice(insert_positions)\n",
        "        # Select a random 'B-' tag entity to insert.\n",
        "        b_tag_entities = {k: v for k, v in entity_pool.items() if k.startswith('B-')}\n",
        "        entity_label = random.choice(list(b_tag_entities.keys()))\n",
        "        entity_to_insert = random.choice(b_tag_entities[entity_label])\n",
        "        entity_tokens = entity_to_insert.split()\n",
        "\n",
        "        # Insert the 'B-' entity at the chosen position.\n",
        "        for token in entity_tokens:\n",
        "            augmented_sentence.insert(insert_position + 1, (token, entity_label))\n",
        "            insert_position += 1  # Update the position for the next token insertion.\n",
        "\n",
        "        i_tag_tokens = []  # Initialize the list to avoid the NameError.\n",
        "        # Optionally insert 'I-' tags following the 'B-' tag, ensuring logical consistency.\n",
        "        i_tag_label = 'I' + entity_label[1:]  # Convert 'B-' tag to 'I-' tag.\n",
        "        if i_tag_label in entity_pool and entity_pool[i_tag_label]:\n",
        "            i_tag_entity = random.choice(entity_pool[i_tag_label])\n",
        "            i_tag_tokens = i_tag_entity.split()\n",
        "            for token in i_tag_tokens:\n",
        "                # Ensure the insertion follows logical consistency rules.\n",
        "                if insert_position < len(augmented_sentence) - 1 and (augmented_sentence[insert_position + 1][1] == 'O' or augmented_sentence[insert_position + 1][1].startswith('I-')):\n",
        "                    augmented_sentence.insert(insert_position + 1, (token, i_tag_label))\n",
        "                    insert_position += 1\n",
        "\n",
        "        # Update insert positions to avoid clustering inserted entities.\n",
        "        insert_positions = [pos for pos in insert_positions if pos < insert_position or pos > insert_position + len(entity_tokens) + len(i_tag_tokens)]\n",
        "\n",
        "    return augmented_sentence"
      ],
      "metadata": {
        "id": "t2IsIVBANCnB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entity_pool = {}\n",
        "i = 0\n",
        "while i < len(train_data):\n",
        "  for word, label in train_data[i]:\n",
        "    if label != 'O':\n",
        "        if label not in entity_pool:\n",
        "            entity_pool[label] = []\n",
        "        if word not in entity_pool[label]:\n",
        "            entity_pool[label].append(word)\n",
        "  i += 1  # Increment i inside the while loop\n",
        "\n",
        "#print(entity_pool)\n",
        "print(len(entity_pool['B-PER']))\n",
        "print(len(entity_pool['I-PER']))\n",
        "print(len(entity_pool['B-ORG']))\n",
        "print(len(entity_pool['I-ORG']))\n",
        "print(len(entity_pool['B-LOC']))\n",
        "print(len(entity_pool['I-LOC']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neX-J-ryNFTR",
        "outputId": "d85d48a7-c6a9-4b47-cb9c-923a434e7ff6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "361\n",
            "415\n",
            "373\n",
            "500\n",
            "338\n",
            "309\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_and_adjust_entities(sentence, words_to_replace, entity_pool):\n",
        "    updated_sentence = []\n",
        "    for i, (word, label) in enumerate(sentence):\n",
        "        if word in words_to_replace:\n",
        "            # Choose a random 'B-PER' entity to replace\n",
        "            b_per_entity = random.choice(entity_pool['B-PER']).split()\n",
        "            for j, entity_word in enumerate(b_per_entity):\n",
        "                # Use 'B-PER' for the first token, 'I-PER' for subsequent tokens\n",
        "                new_label = 'I-PER' if j > 0 else 'B-PER'\n",
        "                updated_sentence.append((entity_word, new_label))\n",
        "        else:\n",
        "            updated_sentence.append((word, label))\n",
        "\n",
        "    # Adjust consecutive entities\n",
        "    adjusted_sentence = []\n",
        "    previous_label = None\n",
        "    for word, label in updated_sentence:\n",
        "        if label == 'B-PER' and previous_label == 'B-PER':\n",
        "            adjusted_label = 'I-PER'\n",
        "        else:\n",
        "            adjusted_label = label\n",
        "        adjusted_sentence.append((word, adjusted_label))\n",
        "        previous_label = label\n",
        "\n",
        "    return adjusted_sentence"
      ],
      "metadata": {
        "id": "L84R8ij2NMm1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_consecutive_entities(sentence):\n",
        "    adjusted_sentence = []\n",
        "    previous_label = None  # Keep track of the previous label\n",
        "\n",
        "    for word, label in sentence:\n",
        "        if label == 'B-PER' and previous_label == 'B-PER':\n",
        "            # Change to 'I-PER' if the previous label was also 'B-PER'\n",
        "            adjusted_label = 'I-PER'\n",
        "        else:\n",
        "            adjusted_label = label\n",
        "\n",
        "        adjusted_sentence.append((word, adjusted_label))\n",
        "        previous_label = label  # Update the previous label for the next iteration\n",
        "\n",
        "    return adjusted_sentence"
      ],
      "metadata": {
        "id": "hU8ZNgUJNPVy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_train_data_v3 = [augment_sentence_with_entities_v3(sentence, entity_pool, num_entities=1) for sentence in train_data]\n",
        "print(train_data[35])\n",
        "print(augmented_train_data_v3[35])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUzUZoMkNcrC",
        "outputId": "722da3da-eb1a-45e8-89de-734a4f4a2c69"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Tagann', 'O'), ('an', 'O'), ('chéad', 'O'), ('cheist', 'O'), ('ón', 'O'), ('Teachta', 'O'), ('Ó', 'B-PER'), ('Cuív', 'I-PER'), ('.', 'O')]\n",
            "[('Tagann', 'O'), ('hArd-Chúirte', 'B-ORG'), ('Chill', 'I-ORG'), ('an', 'O'), ('chéad', 'O'), ('cheist', 'O'), ('ón', 'O'), ('Teachta', 'O'), ('Ó', 'B-PER'), ('Cuív', 'I-PER'), ('.', 'O')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/RDA_training.conll\"\n",
        "\n",
        "with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "    for training in train_data:\n",
        "        for token, tag in training:\n",
        "            file.write(f\"{token}\\t{tag}\\n\")\n",
        "        file.write(\"\\n\")\n",
        "\n",
        "print(f\"Data exported to {file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfmEhxHwOvR3",
        "outputId": "1a5d5d73-8a9d-44b2-dae1-930635bb2a86"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data exported to /content/RDA_training.conll\n"
          ]
        }
      ]
    }
  ]
}